{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T11:25:42.038994Z","iopub.execute_input":"2025-03-10T11:25:42.039213Z","iopub.status.idle":"2025-03-10T11:26:04.378173Z","shell.execute_reply.started":"2025-03-10T11:25:42.039193Z","shell.execute_reply":"2025-03-10T11:26:04.377494Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"dataset = load_dataset(\"squad_v2\")\ntrain_data = dataset[\"train\"].shuffle(seed=42).select(range(15000))\nval_data = dataset[\"validation\"].shuffle(seed=42).select(range(2000))\n\n# Load SpanBERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"SpanBERT/spanbert-large-cased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T11:26:16.548919Z","iopub.execute_input":"2025-03-10T11:26:16.549224Z","iopub.status.idle":"2025-03-10T11:26:22.537899Z","shell.execute_reply.started":"2025-03-10T11:26:16.549200Z","shell.execute_reply":"2025-03-10T11:26:22.536993Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9325c327f4f4082b510ba0f5b467e0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e39c03d610ae492aba785a2beb707539"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"110af3e5f71c481e950f28d9b567218a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c18e3f2d7d84762bfaf666916707e58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11df9c2e8162467aa9c2ddbd6293ca0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b57ac0f075934e3d88fb7b02dff7b8b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67c33e318d3f465893ac90ce59005c43"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def preprocess_data(batch):\n    inputs = tokenizer(batch[\"question\"], batch[\"context\"], truncation=True, padding=\"max_length\", max_length=384)\n\n    start_positions = []\n    end_positions = []\n\n    for answers in batch[\"answers\"]:\n        if answers and len(answers[\"text\"]) > 0:\n            start = answers[\"answer_start\"][0]\n            end = start + len(answers[\"text\"][0])\n        else:\n            start = 0\n            end = 0\n        start_positions.append(start)\n        end_positions.append(end)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n\n    return inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T11:32:37.201314Z","iopub.execute_input":"2025-03-10T11:32:37.201653Z","iopub.status.idle":"2025-03-10T11:32:37.207027Z","shell.execute_reply.started":"2025-03-10T11:32:37.201622Z","shell.execute_reply":"2025-03-10T11:32:37.205964Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_dataset = train_data.map(preprocess_data, batched=True)\nval_dataset = val_data.map(preprocess_data, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T11:32:41.222841Z","iopub.execute_input":"2025-03-10T11:32:41.223128Z","iopub.status.idle":"2025-03-10T11:32:49.347631Z","shell.execute_reply.started":"2025-03-10T11:32:41.223108Z","shell.execute_reply":"2025-03-10T11:32:49.346751Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/15000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"904aa18ae94648b7a29102080e730e57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3dd433f377d4cd4b760101203830044"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\n\n# Ensure CUDA usage by setting device automatically\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load SpanBERT model and tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"SpanBERT/spanbert-large-cased\").to(device)\n\n# Training Arguments (use GPU if available)\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=16,  # Increase batch size for faster training on GPU\n    per_device_eval_batch_size=16,   # Larger batch size accelerates evaluation\n    num_train_epochs=6,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=100,               # Log more frequently\n    save_total_limit=2,\n    report_to=\"tensorboard\",         # For better monitoring\n    fp16=True,                       # Enable mixed precision for faster training\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer\n)\n\n# Start Training\ntrainer.train()\n\n# Save model\nmodel.save_pretrained(\"./spanbert_qa_finetuned\")\ntokenizer.save_pretrained(\"./spanbert_qa_finetuned\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T14:11:54.179926Z","iopub.execute_input":"2025-03-10T14:11:54.180214Z","iopub.status.idle":"2025-03-10T17:21:03.781496Z","shell.execute_reply.started":"2025-03-10T14:11:54.180192Z","shell.execute_reply":"2025-03-10T17:21:03.780500Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-9-6005c2debc66>:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5628' max='5628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5628/5628 3:09:01, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>5.951900</td>\n      <td>5.949219</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>5.951200</td>\n      <td>5.949219</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>5.951400</td>\n      <td>5.949219</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>5.951900</td>\n      <td>5.949219</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>5.951000</td>\n      <td>5.949219</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>5.950200</td>\n      <td>5.949219</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"('./spanbert_qa_finetuned/tokenizer_config.json',\n './spanbert_qa_finetuned/special_tokens_map.json',\n './spanbert_qa_finetuned/vocab.txt',\n './spanbert_qa_finetuned/added_tokens.json',\n './spanbert_qa_finetuned/tokenizer.json')"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, DataCollatorWithPadding\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport json\n\n# Ensure correct device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load SpanBERT model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"./spanbert_qa_finetuned\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"./spanbert_qa_finetuned\").to(device)\n\n# Data collator for dynamic padding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Ensure dataset is properly tokenized\ndef preprocess_data(batch):\n    inputs = tokenizer(batch[\"question\"], batch[\"context\"], truncation=True, padding=\"max_length\", max_length=384)\n\n    start_positions = []\n    end_positions = []\n\n    for answers in batch[\"answers\"]:\n        if answers and len(answers[\"text\"]) > 0:\n            start = answers[\"answer_start\"][0]\n            end = start + len(answers[\"text\"][0])\n        else:\n            start = 0\n            end = 0\n        start_positions.append(start)\n        end_positions.append(end)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n\n    return inputs\n\n# Apply tokenization to validation dataset\ntokenized_val_dataset = val_data.map(preprocess_data, batched=True)\n\n# Ensure only relevant columns are passed to the model\ntokenized_val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"])\n\n# Function to get predictions and references\ndef get_predictions(dataset):\n    dataloader = DataLoader(dataset, batch_size=8, collate_fn=data_collator)\n    model.eval()\n\n    predictions, references = [], []\n\n    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n        with torch.no_grad():\n            inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n            outputs = model(**inputs)\n            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n\n            # Get most probable start and end positions\n            start_idx = torch.argmax(start_logits, dim=1).cpu().numpy()\n            end_idx = torch.argmax(end_logits, dim=1).cpu().numpy()\n\n            for i, (start, end) in enumerate(zip(start_idx, end_idx)):\n                # Decode prediction and store\n                pred_answer = tokenizer.decode(batch[\"input_ids\"][i][start:end + 1], skip_special_tokens=True)\n                predictions.append(pred_answer)\n\n                # Reference answer\n                true_answer = val_data[i][\"answers\"][\"text\"][0] if val_data[i][\"answers\"][\"text\"] else \"\"\n                references.append(true_answer)\n\n    return predictions, references\n\n# Calculate Exact Match (EM) score\ndef exact_match_score(predictions, references):\n    matches = sum(p.strip() == r.strip() for p, r in zip(predictions, references))\n    return matches / len(references) * 100\n\n# Evaluate the model\npreds, refs = get_predictions(tokenized_val_dataset)\nem_score = exact_match_score(preds, refs)\n\nprint(f\"Exact Match (EM) Score: {em_score:.2f}%\")\n\n# Plot loss curve from logs\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:18:42.641565Z","iopub.execute_input":"2025-03-10T18:18:42.641952Z","iopub.status.idle":"2025-03-10T18:19:57.058694Z","shell.execute_reply.started":"2025-03-10T18:18:42.641911Z","shell.execute_reply":"2025-03-10T18:19:57.057370Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d35244d12ed4b86920af21186120283"}},"metadata":{}},{"name":"stderr","text":"Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [01:12<00:00,  3.44it/s]","output_type":"stream"},{"name":"stdout","text":"Exact Match (EM) Score: 35.30%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-64f04ce62338>\u001b[0m in \u001b[0;36m<cell line: 111>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# Display loss curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mplot_loss_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-23-64f04ce62338>\u001b[0m in \u001b[0;36mplot_loss_curve\u001b[0;34m(log_dir)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{log_dir}/trainer_state.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log_history\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './logs/trainer_state.json'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './logs/trainer_state.json'","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os\n\ndef plot_loss_curve_from_tfevents(log_dir=\"./logs\"):\n    train_losses = []\n    epochs = []\n\n    # Iterate through TensorBoard event files\n    for file in os.listdir(log_dir):\n        if \"events.out.tfevents\" in file:\n            event_file = os.path.join(log_dir, file)\n\n            for event in tf.compat.v1.train.summary_iterator(event_file):\n                for value in event.summary.value:\n                    if value.tag == \"train/loss\":  # Corrected tag\n                        train_losses.append(value.simple_value)\n                        epochs.append(len(train_losses))\n\n    if not train_losses:\n        print(\"No training losses found!\")\n        return\n\n    # Plotting the training loss curve\n    plt.plot(epochs, train_losses, label=\"Training Loss\")\n    plt.xlabel(\"Steps\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Loss Curve\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Display the loss curve\nplot_loss_curve_from_tfevents(\"/kaggle/working/logs\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:30:17.755616Z","iopub.execute_input":"2025-03-10T18:30:17.755966Z","iopub.status.idle":"2025-03-10T18:30:18.056598Z","shell.execute_reply.started":"2025-03-10T18:30:17.755939Z","shell.execute_reply":"2025-03-10T18:30:18.055787Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"import tensorflow as tf\nimport os\n\ndef inspect_tfevents(log_dir=\"./logs\"):\n    # Iterate through TensorBoard event files\n    for file in os.listdir(log_dir):\n        if \"events.out.tfevents\" in file:\n            event_file = os.path.join(log_dir, file)\n            print(f\"Inspecting: {event_file}\")\n\n            for event in tf.compat.v1.train.summary_iterator(event_file):\n                for value in event.summary.value:\n                    print(f\"Tag: {value.tag}, Value: {value.simple_value}\")\n\n# Inspect logs\ninspect_tfevents(\"/kaggle/working/logs\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:29:13.628485Z","iopub.execute_input":"2025-03-10T18:29:13.628856Z","iopub.status.idle":"2025-03-10T18:29:13.683852Z","shell.execute_reply.started":"2025-03-10T18:29:13.628828Z","shell.execute_reply":"2025-03-10T18:29:13.683195Z"}},"outputs":[{"name":"stdout","text":"Inspecting: /kaggle/working/logs/events.out.tfevents.1741606418.0a61df013961.31.0\nTag: args/text_summary, Value: 0.0\nTag: model_config/text_summary, Value: 0.0\nInspecting: /kaggle/working/logs/events.out.tfevents.1741615917.0a61df013961.31.1\nTag: args/text_summary, Value: 0.0\nTag: model_config/text_summary, Value: 0.0\nTag: train/loss, Value: 4.910399913787842\nTag: train/grad_norm, Value: 3.1807827949523926\nTag: train/learning_rate, Value: 4.9164889787789434e-05\nTag: train/epoch, Value: 0.10660980641841888\nTag: train/loss, Value: 5.952600002288818\nTag: train/grad_norm, Value: 0.5761380791664124\nTag: train/learning_rate, Value: 4.8276473535224795e-05\nTag: train/epoch, Value: 0.21321961283683777\nTag: train/loss, Value: 5.952400207519531\nTag: train/grad_norm, Value: 0.48687562346458435\nTag: train/learning_rate, Value: 4.7388060920638964e-05\nTag: train/epoch, Value: 0.31982943415641785\nTag: train/loss, Value: 5.9517998695373535\nTag: train/grad_norm, Value: 0.4408471882343292\nTag: train/learning_rate, Value: 4.6499644668074325e-05\nTag: train/epoch, Value: 0.42643922567367554\nTag: train/loss, Value: 5.952099800109863\nTag: train/grad_norm, Value: 0.40619394183158875\nTag: train/learning_rate, Value: 4.5611228415509686e-05\nTag: train/epoch, Value: 0.5330490469932556\nTag: train/loss, Value: 5.953499794006348\nTag: train/grad_norm, Value: 0.38986483216285706\nTag: train/learning_rate, Value: 4.4722815800923854e-05\nTag: train/epoch, Value: 0.6396588683128357\nTag: train/loss, Value: 5.951399803161621\nTag: train/grad_norm, Value: 0.46819332242012024\nTag: train/learning_rate, Value: 4.3834399548359215e-05\nTag: train/epoch, Value: 0.746268630027771\nTag: train/loss, Value: 5.950200080871582\nTag: train/grad_norm, Value: 0.38597601652145386\nTag: train/learning_rate, Value: 4.2945983295794576e-05\nTag: train/epoch, Value: 0.8528784513473511\nTag: train/loss, Value: 5.951900005340576\nTag: train/grad_norm, Value: 0.39638054370880127\nTag: train/learning_rate, Value: 4.2057570681208745e-05\nTag: train/epoch, Value: 0.9594882726669312\nTag: eval/loss, Value: 5.94921875\nTag: eval/runtime, Value: 77.0073013305664\nTag: eval/samples_per_second, Value: 25.972000122070312\nTag: eval/steps_per_second, Value: 1.6230000257492065\nTag: train/epoch, Value: 1.0\nTag: train/loss, Value: 5.952300071716309\nTag: train/grad_norm, Value: 0.5631987452507019\nTag: train/learning_rate, Value: 4.1169154428644106e-05\nTag: train/epoch, Value: 1.0660980939865112\nTag: train/loss, Value: 5.952199935913086\nTag: train/grad_norm, Value: 0.44743385910987854\nTag: train/learning_rate, Value: 4.028073817607947e-05\nTag: train/epoch, Value: 1.1727079153060913\nTag: train/loss, Value: 5.951200008392334\nTag: train/grad_norm, Value: 0.3907361328601837\nTag: train/learning_rate, Value: 3.9392325561493635e-05\nTag: train/epoch, Value: 1.2793177366256714\nTag: train/loss, Value: 5.952000141143799\nTag: train/grad_norm, Value: 0.401574045419693\nTag: train/learning_rate, Value: 3.8503909308928996e-05\nTag: train/epoch, Value: 1.3859275579452515\nTag: train/loss, Value: 5.95389986038208\nTag: train/grad_norm, Value: 0.3389319181442261\nTag: train/learning_rate, Value: 3.761549305636436e-05\nTag: train/epoch, Value: 1.492537260055542\nTag: train/loss, Value: 5.952700138092041\nTag: train/grad_norm, Value: 0.37579768896102905\nTag: train/learning_rate, Value: 3.6727080441778526e-05\nTag: train/epoch, Value: 1.599147081375122\nTag: train/loss, Value: 5.951900005340576\nTag: train/grad_norm, Value: 0.4356859028339386\nTag: train/learning_rate, Value: 3.583866418921389e-05\nTag: train/epoch, Value: 1.7057569026947021\nTag: train/loss, Value: 5.949900150299072\nTag: train/grad_norm, Value: 0.37591779232025146\nTag: train/learning_rate, Value: 3.495024793664925e-05\nTag: train/epoch, Value: 1.8123667240142822\nTag: train/loss, Value: 5.951200008392334\nTag: train/grad_norm, Value: 0.34434840083122253\nTag: train/learning_rate, Value: 3.4061835322063416e-05\nTag: train/epoch, Value: 1.9189765453338623\nTag: eval/loss, Value: 5.94921875\nTag: eval/runtime, Value: 77.40399932861328\nTag: eval/samples_per_second, Value: 25.83799934387207\nTag: eval/steps_per_second, Value: 1.6150000095367432\nTag: train/epoch, Value: 2.0\nTag: train/loss, Value: 5.9517998695373535\nTag: train/grad_norm, Value: 0.37333643436431885\nTag: train/learning_rate, Value: 3.317341906949878e-05\nTag: train/epoch, Value: 2.0255863666534424\nTag: train/loss, Value: 5.951499938964844\nTag: train/grad_norm, Value: 0.3912121057510376\nTag: train/learning_rate, Value: 3.228500281693414e-05\nTag: train/epoch, Value: 2.1321961879730225\nTag: train/loss, Value: 5.9506001472473145\nTag: train/grad_norm, Value: 0.3527510464191437\nTag: train/learning_rate, Value: 3.139659020234831e-05\nTag: train/epoch, Value: 2.2388060092926025\nTag: train/loss, Value: 5.952300071716309\nTag: train/grad_norm, Value: 0.4255184829235077\nTag: train/learning_rate, Value: 3.0508173949783668e-05\nTag: train/epoch, Value: 2.3454158306121826\nTag: train/loss, Value: 5.950300216674805\nTag: train/grad_norm, Value: 0.3852511942386627\nTag: train/learning_rate, Value: 2.961975769721903e-05\nTag: train/epoch, Value: 2.4520256519317627\nTag: train/loss, Value: 5.952000141143799\nTag: train/grad_norm, Value: 0.32586005330085754\nTag: train/learning_rate, Value: 2.8731343263643794e-05\nTag: train/epoch, Value: 2.5586354732513428\nTag: train/loss, Value: 5.952499866485596\nTag: train/grad_norm, Value: 0.3140684962272644\nTag: train/learning_rate, Value: 2.784292883006856e-05\nTag: train/epoch, Value: 2.665245294570923\nTag: train/loss, Value: 5.950200080871582\nTag: train/grad_norm, Value: 0.3520500361919403\nTag: train/learning_rate, Value: 2.695451257750392e-05\nTag: train/epoch, Value: 2.771855115890503\nTag: train/loss, Value: 5.950799942016602\nTag: train/grad_norm, Value: 0.37363725900650024\nTag: train/learning_rate, Value: 2.6066098143928684e-05\nTag: train/epoch, Value: 2.878464937210083\nTag: train/loss, Value: 5.951399803161621\nTag: train/grad_norm, Value: 0.3478136360645294\nTag: train/learning_rate, Value: 2.517768371035345e-05\nTag: train/epoch, Value: 2.985074520111084\nTag: eval/loss, Value: 5.94921875\nTag: eval/runtime, Value: 77.23889923095703\nTag: eval/samples_per_second, Value: 25.893999099731445\nTag: eval/steps_per_second, Value: 1.6180000305175781\nTag: train/epoch, Value: 3.0\nTag: train/loss, Value: 5.951000213623047\nTag: train/grad_norm, Value: 0.337437242269516\nTag: train/learning_rate, Value: 2.428926745778881e-05\nTag: train/epoch, Value: 3.091684341430664\nTag: train/loss, Value: 5.9506001472473145\nTag: train/grad_norm, Value: 0.3035908043384552\nTag: train/learning_rate, Value: 2.3400853024213575e-05\nTag: train/epoch, Value: 3.198294162750244\nTag: train/loss, Value: 5.9506001472473145\nTag: train/grad_norm, Value: 0.3659481108188629\nTag: train/learning_rate, Value: 2.251243859063834e-05\nTag: train/epoch, Value: 3.304903984069824\nTag: train/loss, Value: 5.950900077819824\nTag: train/grad_norm, Value: 0.3364345133304596\nTag: train/learning_rate, Value: 2.16240223380737e-05\nTag: train/epoch, Value: 3.4115138053894043\nTag: train/loss, Value: 5.950799942016602\nTag: train/grad_norm, Value: 0.32825613021850586\nTag: train/learning_rate, Value: 2.0735607904498465e-05\nTag: train/epoch, Value: 3.5181236267089844\nTag: train/loss, Value: 5.950399875640869\nTag: train/grad_norm, Value: 0.33717119693756104\nTag: train/learning_rate, Value: 1.984719347092323e-05\nTag: train/epoch, Value: 3.6247334480285645\nTag: train/loss, Value: 5.952199935913086\nTag: train/grad_norm, Value: 0.3382924199104309\nTag: train/learning_rate, Value: 1.895877721835859e-05\nTag: train/epoch, Value: 3.7313432693481445\nTag: train/loss, Value: 5.94920015335083\nTag: train/grad_norm, Value: 0.32053759694099426\nTag: train/learning_rate, Value: 1.8070362784783356e-05\nTag: train/epoch, Value: 3.8379530906677246\nTag: train/loss, Value: 5.951900005340576\nTag: train/grad_norm, Value: 0.3095492422580719\nTag: train/learning_rate, Value: 1.7181946532218717e-05\nTag: train/epoch, Value: 3.9445629119873047\nTag: eval/loss, Value: 5.94921875\nTag: eval/runtime, Value: 77.30380249023438\nTag: eval/samples_per_second, Value: 25.871999740600586\nTag: eval/steps_per_second, Value: 1.6169999837875366\nTag: train/epoch, Value: 4.0\nTag: train/loss, Value: 5.949699878692627\nTag: train/grad_norm, Value: 0.315073698759079\nTag: train/learning_rate, Value: 1.6293532098643482e-05\nTag: train/epoch, Value: 4.051172733306885\nTag: train/loss, Value: 5.951099872589111\nTag: train/grad_norm, Value: 0.347214937210083\nTag: train/learning_rate, Value: 1.5405117665068246e-05\nTag: train/epoch, Value: 4.157782554626465\nTag: train/loss, Value: 5.950399875640869\nTag: train/grad_norm, Value: 0.3128432035446167\nTag: train/learning_rate, Value: 1.451670232199831e-05\nTag: train/epoch, Value: 4.264392375946045\nTag: train/loss, Value: 5.950900077819824\nTag: train/grad_norm, Value: 0.30615678429603577\nTag: train/learning_rate, Value: 1.3628286978928372e-05\nTag: train/epoch, Value: 4.371002197265625\nTag: train/loss, Value: 5.951499938964844\nTag: train/grad_norm, Value: 0.33306533098220825\nTag: train/learning_rate, Value: 1.2739871635858435e-05\nTag: train/epoch, Value: 4.477612018585205\nTag: train/loss, Value: 5.952300071716309\nTag: train/grad_norm, Value: 0.29908332228660583\nTag: train/learning_rate, Value: 1.18514572022832e-05\nTag: train/epoch, Value: 4.584221839904785\nTag: train/loss, Value: 5.951300144195557\nTag: train/grad_norm, Value: 0.31592363119125366\nTag: train/learning_rate, Value: 1.0963041859213263e-05\nTag: train/epoch, Value: 4.690831661224365\nTag: train/loss, Value: 5.9517998695373535\nTag: train/grad_norm, Value: 0.32665854692459106\nTag: train/learning_rate, Value: 1.0074626516143326e-05\nTag: train/epoch, Value: 4.797441482543945\nTag: train/loss, Value: 5.951000213623047\nTag: train/grad_norm, Value: 0.3263249695301056\nTag: train/learning_rate, Value: 9.18621208256809e-06\nTag: train/epoch, Value: 4.904051303863525\nTag: eval/loss, Value: 5.94921875\nTag: eval/runtime, Value: 77.41259765625\nTag: eval/samples_per_second, Value: 25.836000442504883\nTag: eval/steps_per_second, Value: 1.6150000095367432\nTag: train/epoch, Value: 5.0\nTag: train/loss, Value: 5.949900150299072\nTag: train/grad_norm, Value: 0.341155469417572\nTag: train/learning_rate, Value: 8.297796739498153e-06\nTag: train/epoch, Value: 5.0106611251831055\nTag: train/loss, Value: 5.951099872589111\nTag: train/grad_norm, Value: 0.31424975395202637\nTag: train/learning_rate, Value: 7.409381851175567e-06\nTag: train/epoch, Value: 5.1172709465026855\nTag: train/loss, Value: 5.9506001472473145\nTag: train/grad_norm, Value: 0.3193085491657257\nTag: train/learning_rate, Value: 6.52096650810563e-06\nTag: train/epoch, Value: 5.223880767822266\nTag: train/loss, Value: 5.951700210571289\nTag: train/grad_norm, Value: 0.3055357336997986\nTag: train/learning_rate, Value: 5.632551619783044e-06\nTag: train/epoch, Value: 5.330490589141846\nTag: train/loss, Value: 5.951099872589111\nTag: train/grad_norm, Value: 0.3153313100337982\nTag: train/learning_rate, Value: 4.744136276713107e-06\nTag: train/epoch, Value: 5.437100410461426\nTag: train/loss, Value: 5.9506001472473145\nTag: train/grad_norm, Value: 0.30457431077957153\nTag: train/learning_rate, Value: 3.855721388390521e-06\nTag: train/epoch, Value: 5.543710231781006\nTag: train/loss, Value: 5.951600074768066\nTag: train/grad_norm, Value: 0.2942601442337036\nTag: train/learning_rate, Value: 2.967306272694259e-06\nTag: train/epoch, Value: 5.650320053100586\nTag: train/loss, Value: 5.949900150299072\nTag: train/grad_norm, Value: 0.32352960109710693\nTag: train/learning_rate, Value: 2.0788911569979973e-06\nTag: train/epoch, Value: 5.756929874420166\nTag: train/loss, Value: 5.950900077819824\nTag: train/grad_norm, Value: 0.30372756719589233\nTag: train/learning_rate, Value: 1.1904761549885734e-06\nTag: train/epoch, Value: 5.863539218902588\nTag: train/loss, Value: 5.950200080871582\nTag: train/grad_norm, Value: 0.43051785230636597\nTag: train/learning_rate, Value: 3.0206112455744005e-07\nTag: train/epoch, Value: 5.970149040222168\nTag: eval/loss, Value: 5.94921875\nTag: eval/runtime, Value: 77.32530212402344\nTag: eval/samples_per_second, Value: 25.864999771118164\nTag: eval/steps_per_second, Value: 1.6169999837875366\nTag: train/epoch, Value: 6.0\nTag: train/train_runtime, Value: 11344.4462890625\nTag: train/train_samples_per_second, Value: 7.933000087738037\nTag: train/train_steps_per_second, Value: 0.4959999918937683\nTag: train/total_flos, Value: 6.268772344974541e+16\nTag: train/train_loss, Value: 5.932807922363281\nTag: train/epoch, Value: 6.0\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Load fine-tuned SpanBERT model and tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"./spanbert_qa_finetuned\")\ntokenizer = AutoTokenizer.from_pretrained(\"./spanbert_qa_finetuned\")\n\n# Get predictions and evaluate\npredictions, references = get_predictions(model, val_dataset, tokenizer)\n\nem_score = exact_match_score(predictions, references)\nprint(f\"Exact Match (EM) Score: {em_score:.2f}%\")\n\n# Plot training and validation loss\ntrain_loss = trainer.state.log_history\n\ntrain_losses = [log[\"loss\"] for log in train_loss if \"loss\" in log]\nval_losses = [log[\"eval_loss\"] for log in train_loss if \"eval_loss\" in log]\n\nepochs = list(range(1, len(train_losses) + 1))\n\nplt.figure(figsize=(10, 6))\nplt.plot(epochs, train_losses, label=\"Training Loss\", marker=\"o\")\nplt.plot(epochs, val_losses, label=\"Validation Loss\", marker=\"x\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:12:13.842136Z","iopub.execute_input":"2025-03-10T18:12:13.842552Z","iopub.status.idle":"2025-03-10T18:12:14.546325Z","shell.execute_reply.started":"2025-03-10T18:12:13.842512Z","shell.execute_reply":"2025-03-10T18:12:14.545266Z"}},"outputs":[{"name":"stderr","text":"Generating Predictions:   0%|          | 0/125 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mas_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-0b060840adb4>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Get predictions and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mem_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexact_match_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-9f43ce526b06>\u001b[0m in \u001b[0;36mget_predictions\u001b[0;34m(model, dataset, tokenizer, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Generating Predictions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         batch = pad_without_fast_tokenizer_warning(\n\u001b[0m\u001b[1;32m    272\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpad_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpad_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Restore the state of the warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3378\u001b[0m                 \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3380\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3382\u001b[0m     def create_token_type_ids_from_sequences(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    791\u001b[0m                         \u001b[0;34m\"Please see if a fast version of this tokenizer is available to have this feature available.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m                     ) from e\n\u001b[0;32m--> 793\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    794\u001b[0m                     \u001b[0;34m\"Unable to create tensor, you should probably activate truncation and/or padding with\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m                     \u001b[0;34m\" 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."],"ename":"ValueError","evalue":"Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"!zip -r working_backup.zip /kaggle/working\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T19:00:27.038999Z","iopub.execute_input":"2025-03-10T19:00:27.039325Z","iopub.status.idle":"2025-03-10T19:08:12.120129Z","shell.execute_reply.started":"2025-03-10T19:00:27.039296Z","shell.execute_reply":"2025-03-10T19:08:12.119202Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/logs/ (stored 0%)\n  adding: kaggle/working/logs/events.out.tfevents.1741606418.0a61df013961.31.0 (deflated 61%)\n  adding: kaggle/working/logs/events.out.tfevents.1741615917.0a61df013961.31.1 (deflated 65%)\n  adding: kaggle/working/results/ (stored 0%)\n  adding: kaggle/working/results/checkpoint-4690/ (stored 0%)\n  adding: kaggle/working/results/checkpoint-4690/tokenizer.json (deflated 70%)\n  adding: kaggle/working/results/checkpoint-4690/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/results/checkpoint-4690/config.json (deflated 47%)\n  adding: kaggle/working/results/checkpoint-4690/trainer_state.json (deflated 76%)\n  adding: kaggle/working/results/checkpoint-4690/optimizer.pt (deflated 33%)\n  adding: kaggle/working/results/checkpoint-4690/scheduler.pt (deflated 55%)\n  adding: kaggle/working/results/checkpoint-4690/model.safetensors (deflated 8%)\n  adding: kaggle/working/results/checkpoint-4690/training_args.bin (deflated 51%)\n  adding: kaggle/working/results/checkpoint-4690/tokenizer_config.json (deflated 74%)\n  adding: kaggle/working/results/checkpoint-4690/vocab.txt (deflated 49%)\n  adding: kaggle/working/results/checkpoint-4690/rng_state.pth (deflated 25%)\n  adding: kaggle/working/results/checkpoint-5628/ (stored 0%)\n  adding: kaggle/working/results/checkpoint-5628/tokenizer.json (deflated 70%)\n  adding: kaggle/working/results/checkpoint-5628/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/results/checkpoint-5628/config.json (deflated 47%)\n  adding: kaggle/working/results/checkpoint-5628/trainer_state.json (deflated 77%)\n  adding: kaggle/working/results/checkpoint-5628/optimizer.pt (deflated 32%)\n  adding: kaggle/working/results/checkpoint-5628/scheduler.pt (deflated 55%)\n  adding: kaggle/working/results/checkpoint-5628/model.safetensors (deflated 8%)\n  adding: kaggle/working/results/checkpoint-5628/training_args.bin (deflated 51%)\n  adding: kaggle/working/results/checkpoint-5628/tokenizer_config.json (deflated 74%)\n  adding: kaggle/working/results/checkpoint-5628/vocab.txt (deflated 49%)\n  adding: kaggle/working/results/checkpoint-5628/rng_state.pth (deflated 25%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/spanbert_qa_finetuned/ (stored 0%)\n  adding: kaggle/working/spanbert_qa_finetuned/tokenizer.json (deflated 70%)\n  adding: kaggle/working/spanbert_qa_finetuned/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/spanbert_qa_finetuned/config.json (deflated 47%)\n  adding: kaggle/working/spanbert_qa_finetuned/model.safetensors (deflated 8%)\n  adding: kaggle/working/spanbert_qa_finetuned/tokenizer_config.json (deflated 74%)\n  adding: kaggle/working/spanbert_qa_finetuned/vocab.txt (deflated 49%)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:58:05.312391Z","iopub.execute_input":"2025-03-10T18:58:05.312719Z","iopub.status.idle":"2025-03-10T18:58:05.525055Z","shell.execute_reply.started":"2025-03-10T18:58:05.312683Z","shell.execute_reply":"2025-03-10T18:58:05.524052Z"}},"outputs":[{"name":"stdout","text":"mv: cannot stat 'working_backup.zip': No such file or directory\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(\"/kaggle/working/working_backup.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T19:08:12.121628Z","iopub.execute_input":"2025-03-10T19:08:12.121916Z","iopub.status.idle":"2025-03-10T19:08:12.127352Z","shell.execute_reply.started":"2025-03-10T19:08:12.121891Z","shell.execute_reply":"2025-03-10T19:08:12.126761Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/working_backup.zip","text/html":"<a href='/kaggle/working/working_backup.zip' target='_blank'>/kaggle/working/working_backup.zip</a><br>"},"metadata":{}}],"execution_count":30}]}